{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "l5BIr6Gp9VAa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Plugin loading"
      ],
      "metadata": {
        "id": "cmXfi5Nc8nbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAlluZG43en-",
        "outputId": "7cfbef7a-dc75-400c-8e37-ae0a7f7b3f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-0syocffq\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-0syocffq\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install and load Cuda plugin for Google colab\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jtSBxvF6_0p",
        "outputId": "e11d45b2-9c1a-4635-d71e-b7aefc428bb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmphidb9bgs\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1xoIPDI1p7E",
        "outputId": "aa4f2801-080a-4674-c95e-5709bc8d8dde"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 12 21:47:20 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "Ykp5k1TJ88OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <cuda.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MATRIX_SIZE 8192\n",
        "#define MASK_SIZE 3\n",
        "#define CHECK_MALLOC(ptr) {if(ptr == NULL) {printf(\"Not enough memory!\");exit(1);}}\n",
        "\n",
        "__constant__ int mask_device[MASK_SIZE*MASK_SIZE];\n",
        "\n",
        "void\n",
        "cudaInfo() {\n",
        "    // retrieve some info about the CUDA device\n",
        "    int nDevices;\n",
        "    cudaGetDeviceCount(&nDevices);\n",
        "    for (int i = 0; i < nDevices; i++) {\n",
        "      cudaDeviceProp prop;\n",
        "      cudaGetDeviceProperties(&prop, i);\n",
        "      printf(\"Device Number: %d\\n\", i);\n",
        "      printf(\"  Device name: %s\\n\", prop.name);\n",
        "      printf(\"  max Blocks Per MultiProcessor: %d\\n\", prop.maxBlocksPerMultiProcessor);\n",
        "      printf(\"  max Threads Per MultiProcessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
        "      printf(\"  max Threads Per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
        "      printf(\"  num SM: %d\\n\", prop.multiProcessorCount);\n",
        "      printf(\"  num bytes sharedMem Per Block: %d\\n\", prop.sharedMemPerBlock);\n",
        "      printf(\"  num bytes sharedMem Per Multiprocessor: %d\\n\", prop.sharedMemPerMultiprocessor);\n",
        "      printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "           prop.memoryClockRate);\n",
        "      printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "           prop.memoryBusWidth);\n",
        "      printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "           2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    }\n",
        "}\n",
        "\n",
        "bool\n",
        "areEqual(int* mat1, int* mat2, int mat_dim) {\n",
        "    for(int i = 0; i < mat_dim; ++i) {\n",
        "        for(int j = 0; j < mat_dim; ++j) {\n",
        "            if(mat1[j+mat_dim*i] != mat2[j+mat_dim*i]) {\n",
        "                printf(\"[INFO] Test failed at (%d,%d)\", i, j);\n",
        "                return false;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    return true;\n",
        "}\n",
        "\n",
        "void\n",
        "initMatrix(\n",
        "    int* mat,\n",
        "    int mat_dim\n",
        ") {\n",
        "    for(int i = 0; i < mat_dim; ++i) {\n",
        "        mat[i] = 0;\n",
        "    }\n",
        "}\n",
        "\n",
        "void\n",
        "convCpu(\n",
        "    int* mat_in,\n",
        "    int mat_dim,\n",
        "    int* mask,\n",
        "    int mask_dim,\n",
        "    int* mat_out\n",
        ") {\n",
        "    int half_mask = mask_dim/2;\n",
        "    for(int i = 0; i < mat_dim; ++i) {\n",
        "        for(int j = 0; j < mat_dim; ++j) {\n",
        "            mat_out[j + i*mat_dim] = 0;\n",
        "            for(int k = -half_mask; k <= half_mask; ++k) {\n",
        "                for(int l = -half_mask; l <= half_mask; ++l) {\n",
        "                    if(j+l >= 0 && j+l < mat_dim && i+k >= 0 && i+k < mat_dim) {\n",
        "                        mat_out[j + i*mat_dim] +=\n",
        "                            mat_in[(j+l) + (i+k)*mat_dim] *\n",
        "                            mask[(half_mask+l) + (half_mask+k)*mask_dim];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void\n",
        "convKernel2D(\n",
        "    int* mat_in,\n",
        "    int mat_dim,\n",
        "    int mask_dim,\n",
        "    int* mat_out\n",
        ") {\n",
        "    int row = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "    int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "\n",
        "    // Boundaries check (We might have more threads than strictly necessary)\n",
        "    if(row < mat_dim && col < mat_dim) {\n",
        "        int half_mask = mask_dim/2;\n",
        "        int tmp_sum = 0;\n",
        "        for(int i = -half_mask; i <= half_mask; ++i) {\n",
        "            for(int j = -half_mask; j <= half_mask; ++j) {\n",
        "                if(col+j >= 0 && col+j < mat_dim && row+i >= 0 && row+i < mat_dim) {\n",
        "                    tmp_sum +=\n",
        "                        mat_in[(col+j) + (row+i)*mat_dim] *\n",
        "                        mask_device[(half_mask+j)+(half_mask+i)*mask_dim];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        mat_out[col + row*mat_dim] = tmp_sum;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "__global__ void\n",
        "convKernel2DTiling(\n",
        "    int* mat_in,\n",
        "    int mat_dim,\n",
        "    int mask_dim,\n",
        "    int* mat_out\n",
        ") {\n",
        "    // All the threads collaborate to load a tile into the shared memory\n",
        "    extern __shared__ int tile[];\n",
        "\n",
        "    int tx = threadIdx.x;\n",
        "    int ty = threadIdx.y;\n",
        "    int offset = (mask_dim-1)/2;\n",
        "    // int col_i = blockIdx.x*blockDim.x - offset + tx;\n",
        "    // int row_i = blockIdx.y*blockDim.y - offset + ty;\n",
        "    int col_i = blockIdx.x*(blockDim.x-2*offset) - offset + tx;\n",
        "    int row_i = blockIdx.y*(blockDim.y-2*offset) - offset + ty;\n",
        "\n",
        "    if(row_i >= 0 && row_i < mat_dim && col_i >= 0 && col_i < mat_dim) {\n",
        "        tile[tx + blockDim.x * ty] = mat_in[col_i + mat_dim * row_i];\n",
        "    } else {\n",
        "        tile[tx + blockDim.x * ty] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Only certain threads are going to produce an output\n",
        "    if(tx >= offset && ty >= offset && tx < (blockDim.x-offset) && ty < (blockDim.y-offset)) {\n",
        "        int half_mask = mask_dim/2;\n",
        "        int tmp_sum = 0;\n",
        "        for(int i = -half_mask; i <= half_mask; ++i) {\n",
        "            for(int j = -half_mask; j <= half_mask; ++j) {\n",
        "                tmp_sum +=\n",
        "                    tile[(tx+j) + (ty+i)*blockDim.x] *\n",
        "                    mask_device[(half_mask+j)+(half_mask+i)*mask_dim];\n",
        "            }\n",
        "        }\n",
        "        // Boundary check for output writing\n",
        "        if(col_i < mat_dim && row_i < mat_dim) {\n",
        "            mat_out[col_i + mat_dim * row_i] = tmp_sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char const *argv[])\n",
        "{\n",
        "    cudaInfo();\n",
        "\n",
        "    int matrix_num_el = MATRIX_SIZE*MATRIX_SIZE;\n",
        "    int mask_num_el = MASK_SIZE*MASK_SIZE;\n",
        "\n",
        "    // Allocate host memory and initialize it\n",
        "    int* mat_in = (int*)malloc(matrix_num_el*sizeof(int));\n",
        "    CHECK_MALLOC(mat_in);\n",
        "    int* mat_out = (int*)malloc(matrix_num_el*sizeof(int));\n",
        "    CHECK_MALLOC(mat_out);\n",
        "    int* mask = (int*)malloc(mask_num_el*sizeof(int));\n",
        "    CHECK_MALLOC(mask);\n",
        "\n",
        "    for(int i = 0; i < matrix_num_el; ++i) {\n",
        "        mat_in[i] = 2;\n",
        "    }\n",
        "\n",
        "    for(int i = 0; i < mask_num_el; ++i) {\n",
        "        mask[i] = 10;\n",
        "    }\n",
        "\n",
        "    // ************ CPU COMPUTATION **************\n",
        "    int* mat_out_tmp = (int*)malloc(matrix_num_el*sizeof(int));\n",
        "    CHECK_MALLOC(mat_out_tmp);\n",
        "\n",
        "    clock_t begin = clock();\n",
        "    convCpu(mat_in, MATRIX_SIZE, mask, MASK_SIZE, mat_out_tmp);\n",
        "    clock_t end = clock();\n",
        "\n",
        "    double time_spent = ((double)((end - begin)) * 1000) / CLOCKS_PER_SEC;\n",
        "    printf(\"\\n\\n[CPU] matrix_size: %dx%d\\n\", MATRIX_SIZE, MATRIX_SIZE);\n",
        "    printf(\"  - CPU time: %f ms\\n\", time_spent);\n",
        "    // *******************************************\n",
        "\n",
        "\n",
        "\n",
        "    // ************ GPU COMPUTATION **************\n",
        "    cudaError_t error;\n",
        "\n",
        "    // Allocate device memory and initialize it\n",
        "    int* mat_in_device;\n",
        "    cudaMalloc(&mat_in_device, matrix_num_el*sizeof(int));\n",
        "    cudaMemcpy(mat_in_device, mat_in, matrix_num_el*sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int* mat_out_device;\n",
        "    cudaMalloc(&mat_out_device, matrix_num_el*sizeof(int));\n",
        "\n",
        "    cudaMemcpyToSymbol(mask_device, mask, mask_num_el*sizeof(int));\n",
        "\n",
        "    printf(\"\\n\\n[GPU] matrix_size: %dx%d\\n\", MATRIX_SIZE, MATRIX_SIZE);\n",
        "    // Test for different block sizes\n",
        "    for(int i = 4; i <= 32; i*=2) {\n",
        "        printf(\"[GPU] block_size: %d\\n\", i);\n",
        "        // Kernel parameters\n",
        "        int thread_num_x = i;\n",
        "        int thread_num_y = thread_num_x;\n",
        "\n",
        "        int block_num_x = (MATRIX_SIZE) / thread_num_x;\n",
        "        int block_num_y = (MATRIX_SIZE) / thread_num_y;\n",
        "\n",
        "        dim3 grid_dim(block_num_x,block_num_y);\n",
        "        dim3 block_dim(thread_num_x, thread_num_y);\n",
        "\n",
        "        // In the tiled algorithm the number of threads per block is increased\n",
        "        //  by a certain number to account for the halo\n",
        "        dim3 grid_dim_tiling(block_num_x, block_num_y);\n",
        "        int thread_num_x_tiling = thread_num_x+MASK_SIZE-1;\n",
        "        int thread_num_y_tiling = thread_num_y+MASK_SIZE-1;\n",
        "        dim3 block_dim_tiling(thread_num_x_tiling,thread_num_y_tiling);\n",
        "\n",
        "        // Performance measurment objects\n",
        "        cudaEvent_t start, stop;\n",
        "        cudaEventCreate(&start);\n",
        "        cudaEventCreate(&stop);\n",
        "\n",
        "        cudaEventRecord(start, 0);\n",
        "        convKernel2D<<<grid_dim, block_dim>>>\n",
        "         (mat_in_device, MATRIX_SIZE, MASK_SIZE, mat_out_device);\n",
        "        /*\n",
        "        convKernel2DTiling<<<grid_dim_tiling,\n",
        "                             block_dim_tiling,\n",
        "                             thread_num_x_tiling*thread_num_y_tiling*sizeof(int)>>>\n",
        "                             (mat_in_device, MATRIX_SIZE, MASK_SIZE, mat_out_device);\n",
        "        */\n",
        "        cudaDeviceSynchronize();\n",
        "        error = cudaGetLastError();\n",
        "        if(error != cudaSuccess) {\n",
        "            printf(\"[INFO] Couldn't launch kernel\");\n",
        "            exit(1);\n",
        "        }\n",
        "        cudaEventRecord(stop, 0);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        float gpu_elapsed_time_ms = 0.0;\n",
        "        cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);\n",
        "        printf(\"  - GPU time: %f ms\\n\",\n",
        "               gpu_elapsed_time_ms, i);\n",
        "\n",
        "        // Copy back data to device\n",
        "        cudaMemcpy(mat_out, mat_out_device, matrix_num_el*sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Check results\n",
        "        if(areEqual(mat_out_tmp, mat_out, MATRIX_SIZE)) {\n",
        "            printf(\"  - Test passed\\n\", i);\n",
        "        } else {\n",
        "            printf(\"  - Test failed\\n\", i);\n",
        "        }\n",
        "\n",
        "        initMatrix(mat_out, matrix_num_el);\n",
        "    }\n",
        "    // *******************************************\n",
        "\n",
        "    int index = (0)+(88)*MATRIX_SIZE;\n",
        "    printf(\"mat_out[%d] = %d\", index, mat_out[index]);\n",
        "\n",
        "    free(mat_in);\n",
        "    free(mat_out);\n",
        "    free(mat_out_tmp);\n",
        "    free(mask);\n",
        "\n",
        "    cudaFree(mat_in_device);\n",
        "    cudaFree(mat_out_device);\n",
        "    cudaFree(mask_device);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "Nnkj6Opc3xJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c46d665-95a9-418f-bae4-586bfaeb4e87"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  max Blocks Per MultiProcessor: 16\n",
            "  max Threads Per MultiProcessor: 1024\n",
            "  max Threads Per Block: 1024\n",
            "  num SM: 40\n",
            "  num bytes sharedMem Per Block: 49152\n",
            "  num bytes sharedMem Per Multiprocessor: 65536\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "\n",
            "\n",
            "[CPU] matrix_size: 8192x8192\n",
            "  - CPU time: 6129.021000 ms\n",
            "\n",
            "\n",
            "[GPU] matrix_size: 8192x8192\n",
            "[GPU] block_size: 4\n",
            "  - GPU time: 23.554079 ms\n",
            "  - Test passed\n",
            "[GPU] block_size: 8\n",
            "  - GPU time: 8.770016 ms\n",
            "  - Test passed\n",
            "[GPU] block_size: 16\n",
            "  - GPU time: 7.171264 ms\n",
            "  - Test passed\n",
            "[GPU] block_size: 32\n",
            "  - GPU time: 8.040768 ms\n",
            "  - Test passed\n",
            "mat_out[720896] = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scratch"
      ],
      "metadata": {
        "id": "l5BIr6Gp9VAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Matrix multiply example\n",
        "for(block_size= 4; block_size <= 32; block_size *= 2)\n",
        "{\n",
        "    int *a, *b, *c;\n",
        "    cudaMallocManaged((void **) &a, sizeof(int)*MATRIX_SIZE*MATRIX_SIZE);\n",
        "    cudaMallocManaged((void **) &b, sizeof(int)*MATRIX_SIZE*MATRIX_SIZE);\n",
        "    cudaMallocManaged((void **) &c, sizeof(int)*MATRIX_SIZE*MATRIX_SIZE);\n",
        "\n",
        "    // initialize matrix A\n",
        "    for (int i = 0; i < MATRIX_SIZE; ++i) {\n",
        "        for (int j = 0; j < MATRIX_SIZE; ++j) {\n",
        "            a[i * MATRIX_SIZE + j] = 2;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // initialize matrix B\n",
        "    for (int i = 0; i < MATRIX_SIZE; ++i) {\n",
        "        for (int j = 0; j < MATRIX_SIZE; ++j) {\n",
        "            b[i * MATRIX_SIZE + j] = 3;\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    float  naive_gpu_elapsed_time_ms;\n",
        "\n",
        "    // some events to count the execution time\n",
        "    //clock_t st, end;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "\n",
        "    unsigned int grid_rows = (MATRIX_SIZE + block_size - 1) / block_size;\n",
        "    unsigned int grid_cols = (MATRIX_SIZE + block_size - 1) / block_size;\n",
        "    dim3 dimGrid(grid_cols, grid_rows);\n",
        "    dim3 dimBlock(block_size, block_size);\n",
        "\n",
        "\n",
        "    cudaEventRecord(start, 0);\n",
        "    gpu_matrix_mult<<<dimGrid, dimBlock>>>(a, b, c, MATRIX_SIZE);\n",
        "    cudaThreadSynchronize();\n",
        "\n",
        "    // time counting terminate\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // compute time elapsed on GPU computing\n",
        "    cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n",
        "    printf(\"Time elapsed on naive GPU matrix multiplication of %dx%d . %dx%d (%d): %f ms.\\n\\n\", MATRIX_SIZE, MATRIX_SIZE, MATRIX_SIZE, MATRIX_SIZE, block_size, naive_gpu_elapsed_time_ms);\n",
        "\n",
        "\n",
        "    // free memory\n",
        "    cudaFree(a);\n",
        "    cudaFree(b);\n",
        "    cudaFree(c);\n",
        "}"
      ],
      "metadata": {
        "id": "1hhJGyar9Xig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}